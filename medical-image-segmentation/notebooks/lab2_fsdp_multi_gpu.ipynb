{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Multi-GPU Training with FSDP on SageMaker\n",
    "\n",
    "## Overview\n",
    "Learn how to scale medical image segmentation training across multiple GPUs using Fully Sharded Data Parallel (FSDP). This enables training larger models that don't fit on a single GPU.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand FSDP vs DDP for distributed training\n",
    "- Configure multi-GPU SageMaker training jobs\n",
    "- Train large models (SwinUNETR) with memory efficiency\n",
    "- Monitor distributed training with TensorBoard, MLflow, and WandB\n",
    "\n",
    "## Prerequisites\n",
    "- Completed Lab 1 (Single GPU Training)\n",
    "- Understanding of distributed training concepts\n",
    "- MLflow and WandB accounts (optional)\n",
    "\n",
    "**Estimated Time:** 45-60 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is FSDP?\n",
    "\n",
    "**Fully Sharded Data Parallel (FSDP)** shards model parameters, gradients, and optimizer states across GPUs:\n",
    "\n",
    "| Feature | DDP | FSDP |\n",
    "|---------|-----|------|\n",
    "| Model Replication | Full copy per GPU | Sharded across GPUs |\n",
    "| Memory Efficiency | Low | High |\n",
    "| Best For | Models < 1B params | Models > 1B params |\n",
    "| Communication | Gradients only | Parameters + Gradients |\n",
    "\n",
    "**Use FSDP when:**\n",
    "- Model doesn't fit in single GPU memory\n",
    "- Training large transformer models (SwinUNETR, ViT)\n",
    "- Need to maximize batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker==2.200.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch.estimator import PyTorch\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session(boto3.Session(region_name='us-east-1'))\n",
    "# sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_region_name\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure Data and Output Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'YOUR_SAGEMAKER_BUCKET_NAME'  # Replace with your actual bucket name\n",
    "data_path = f's3://{bucket}/segmentation_data/'\n",
    "output_path = f's3://{bucket}/segmentation_data/output'\n",
    "\n",
    "print(f\"Training data: {data_path}\")\n",
    "print(f\"Output path: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure Experiment Tracking (Optional)\n",
    "\n",
    "Enable MLflow and/or WandB for advanced experiment tracking.\n",
    "\n",
    "Create an MLFlow tracking server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to enable tracking\n",
    "use_mlflow = True\n",
    "use_wandb = False\n",
    "\n",
    "# MLflow configuration\n",
    "# Create an MLflow tracking server in SageMaker and provide its URI here\n",
    "# mlflow_tracking_uri = \"arn:aws:sagemaker:us-east-1:xxxxx\"  # e.g., \"http://mlflow-server:5000\"\n",
    "mlflow_tracking_uri='./mlruns'  # Local path for testing\n",
    "mlflow_experiment_name = \"medical-segmentation-fsdp\"\n",
    "\n",
    "# WandB configuration\n",
    "wandb_project = \"medical-segmentation-fsdp\"\n",
    "wandb_api_key = \"\"  # Your WandB API key\n",
    "\n",
    "print(f\"MLflow: {'Enabled' if use_mlflow else 'Disabled'}\")\n",
    "print(f\"WandB: {'Enabled' if use_wandb else 'Disabled'}\")\n",
    "print(f\"TensorBoard: Always Enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Hyperparameters\n",
    "\n",
    "Configure for multi-GPU training with larger model (SwinUNETR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"model_name\": \"SegResNet\",  # Larger transformer-based model\n",
    "    \"batch_size\": 2,\n",
    "    \"epochs\": 20,\n",
    "    \"lr\": 1e-4,\n",
    "    \"use_mlflow\": use_mlflow,\n",
    "    \"use_wandb\": use_wandb,\n",
    "}\n",
    "\n",
    "if use_mlflow:\n",
    "    hyperparameters[\"mlflow_tracking_uri\"] = mlflow_tracking_uri\n",
    "    hyperparameters[\"mlflow_experiment_name\"] = mlflow_experiment_name\n",
    "\n",
    "if use_wandb:\n",
    "    hyperparameters[\"wandb_project\"] = wandb_project\n",
    "    hyperparameters[\"wandb_api_key\"] = wandb_api_key\n",
    "\n",
    "print(\"Hyperparameters:\")\n",
    "for key, value in hyperparameters.items():\n",
    "    if 'api_key' not in key:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Multi-GPU Estimator\n",
    "\n",
    "Configure for 4 GPUs using ml.g5.12xlarge:\n",
    "- 4x NVIDIA A10G GPUs (96GB total GPU memory)\n",
    "- 192GB system memory\n",
    "- Ideal for large model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    entry_point=\"train_fsdp_all.py\",\n",
    "    source_dir=\"../code/training\", # leave the requirements.txt in the source_dir\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.g4dn.12xlarge\",  # 4 GPUs\n",
    "    framework_version=\"2.1.0\",\n",
    "    py_version=\"py310\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=output_path,\n",
    "    base_job_name=\"medical-seg-fsdp\",\n",
    "    keep_alive_period_in_seconds=1800,\n",
    "    environment={\n",
    "        \"PIP_CACHE_DIR\": \"/opt/ml/sagemaker/warmpoolcache/pip\",\n",
    "        \"NCCL_DEBUG\": \"INFO\"  # Enable NCCL debugging\n",
    "    },\n",
    "    distribution={\n",
    "        \"pytorchddp\": {\n",
    "            \"enabled\": True\n",
    "        }\n",
    "    },\n",
    "    # dependencies=['requirements.txt'], not needed as included in source_dir\n",
    "    sagemaker_session = sagemaker_session,\n",
    ")\n",
    "\n",
    "print(\"✓ Multi-GPU Estimator created!\")\n",
    "print(\"  Instance: ml.g4dn.12xlarge (4 GPUs)\")\n",
    "print(\"  Model: SwinUNETR\")\n",
    "print(\"  Strategy: FSDP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Launch Distributed Training\n",
    "\n",
    "Start FSDP training across 4 GPUs.\n",
    "\n",
    "**Expected Duration:** 30-60 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({\"training\": data_path}, wait=True, logs=\"All\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Analyze Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name = estimator.latest_training_job.name\n",
    "model_data = estimator.model_data\n",
    "\n",
    "print(f\"Training Job: {training_job_name}\")\n",
    "print(f\"Model Artifacts: {model_data}\")\n",
    "print(f\"\\nView logs in CloudWatch:\")\n",
    "print(f\"  Log Group: /aws/sagemaker/TrainingJobs\")\n",
    "print(f\"  Log Stream: {training_job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Download and Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model artifacts\n",
    "!aws s3 cp {model_data} ./model.tar.gz\n",
    "!tar -xzf model.tar.gz\n",
    "\n",
    "print(\"✓ Model downloaded\")\n",
    "print(\"\\nAvailable files:\")\n",
    "!ls -lh *.pth\n",
    "\n",
    "print(\"\\nLaunch TensorBoard:\")\n",
    "print(\"  tensorboard --logdir=./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Compare with Single GPU Training\n",
    "\n",
    "Analyze performance improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "\n",
    "# Get training job details\n",
    "job_details = sm_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "\n",
    "training_time = job_details['TrainingTimeInSeconds']\n",
    "billable_time = job_details['BillableTimeInSeconds']\n",
    "\n",
    "print(f\"Training Time: {training_time/60:.1f} minutes\")\n",
    "print(f\"Billable Time: {billable_time/60:.1f} minutes\")\n",
    "print(f\"\\nEstimated Cost:\")\n",
    "print(f\"  ml.g5.12xlarge: ${(billable_time/3600) * 7.09:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding FSDP Performance\n",
    "\n",
    "### Memory Efficiency\n",
    "```\n",
    "Single GPU (24GB):  Model (20GB) + Batch (4GB) = OOM ❌\n",
    "FSDP 4 GPUs:        Model (5GB/GPU) + Batch (4GB/GPU) = ✓\n",
    "```\n",
    "\n",
    "### Training Speed\n",
    "- **Linear Scaling**: 4 GPUs ≈ 3.5x faster (90% efficiency)\n",
    "- **Communication Overhead**: ~10-15% for FSDP\n",
    "- **Optimal Batch Size**: 2-4 per GPU\n",
    "\n",
    "### When to Use FSDP\n",
    "✓ Model > 10GB  \n",
    "✓ Need larger batch sizes  \n",
    "✓ Training time > 1 hour  \n",
    "✓ Budget allows multi-GPU instances  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "✓ **FSDP Benefits:**\n",
    "- Train models that don't fit on single GPU\n",
    "- 3-4x faster training with 4 GPUs\n",
    "- Memory efficient for large models\n",
    "\n",
    "✓ **Best Practices:**\n",
    "- Start with 2-4 GPUs for cost efficiency\n",
    "- Monitor GPU utilization (aim for >80%)\n",
    "- Use gradient checkpointing for even larger models\n",
    "- Enable mixed precision (FP16) for speed\n",
    "\n",
    "✓ **Monitoring:**\n",
    "- TensorBoard: Always available\n",
    "- MLflow: Experiment tracking and model registry\n",
    "- WandB: Real-time collaboration and visualization\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Lab 3**: Advanced experiment tracking with WandB\n",
    "- Optimize hyperparameters with SageMaker Automatic Model Tuning\n",
    "- Deploy model with SageMaker Inference\n",
    "\n",
    "## Cost Comparison\n",
    "\n",
    "| Instance | GPUs | Cost/Hour | 1hr Training |\n",
    "|----------|------|-----------|-------------|\n",
    "| ml.g5.xlarge | 1 | $1.41 | $1.41 |\n",
    "| ml.g5.12xlarge | 4 | $7.09 | $7.09 |\n",
    "| **Speedup** | **4x** | **5x cost** | **~2x cost** |\n",
    "\n",
    "**Recommendation**: Use multi-GPU for production training, single GPU for prototyping."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seg-jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
