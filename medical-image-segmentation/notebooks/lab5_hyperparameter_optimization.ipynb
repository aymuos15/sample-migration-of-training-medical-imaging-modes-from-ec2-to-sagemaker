{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Hyperparameter Optimization with SageMaker\n",
    "\n",
    "## Overview\n",
    "Learn how to automatically find the best hyperparameters for medical image segmentation using SageMaker Automatic Model Tuning. This lab uses Bayesian optimization to efficiently search the hyperparameter space.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand hyperparameter tuning strategies\n",
    "- Configure SageMaker HyperparameterTuner\n",
    "- Define search spaces and objective metrics\n",
    "- Analyze tuning job results\n",
    "- Select and deploy the best model\n",
    "\n",
    "## Prerequisites\n",
    "- Completed Lab 1 (Single GPU Training)\n",
    "- Understanding of hyperparameters\n",
    "- Budget for multiple training jobs\n",
    "\n",
    "**Estimated Time:** 90-120 minutes  \n",
    "**Estimated Cost:** $15-25 (20 training jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Strategies\n",
    "\n",
    "### Grid Search\n",
    "```\n",
    "lr = [1e-3, 1e-4, 1e-5]\n",
    "batch_size = [2, 4, 8]\n",
    "Total jobs: 3 √ó 3 = 9\n",
    "```\n",
    "‚úì Exhaustive  \n",
    "‚úó Expensive  \n",
    "‚úó Doesn't scale  \n",
    "\n",
    "### Random Search\n",
    "```\n",
    "lr ~ Uniform(1e-5, 1e-3)\n",
    "batch_size ~ Choice([2, 4, 8])\n",
    "Total jobs: User defined\n",
    "```\n",
    "‚úì Better coverage  \n",
    "‚úì Scalable  \n",
    "‚úó No learning  \n",
    "\n",
    "### Bayesian Optimization (SageMaker)\n",
    "```\n",
    "Uses previous results to guide search\n",
    "Balances exploration vs exploitation\n",
    "```\n",
    "‚úì Most efficient  \n",
    "‚úì Learns from results  \n",
    "‚úì Fewer jobs needed  \n",
    "\n",
    "**This Lab Uses:** Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.tuner import (\n",
    "    HyperparameterTuner,\n",
    "    ContinuousParameter,\n",
    "    CategoricalParameter,\n",
    "    IntegerParameter\n",
    ")\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session(boto3.Session(region_name='us-east-1'))\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_region_name\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'your-s3-bucket-name'  # Replace with your S3 bucket name\n",
    "data_path = f's3://{bucket}/segmentation_data/'\n",
    "output_path = f's3://{bucket}/segmentation_data/output'\n",
    "\n",
    "print(f\"Training data: {data_path}\")\n",
    "print(f\"Output path: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Base Estimator\n",
    "\n",
    "Create the base training job configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static hyperparameters (not tuned)\n",
    "static_hyperparameters = {\n",
    "    \"model_name\": \"SegResNet\",\n",
    "    \"epochs\": 10,  # Shorter for faster tuning\n",
    "    \"val_interval\": 2\n",
    "}\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train_simple.py\",\n",
    "    source_dir=\"../code/training\",\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.g5.xlarge\",\n",
    "    framework_version=\"2.1.0\",\n",
    "    py_version=\"py310\",\n",
    "    hyperparameters=static_hyperparameters,\n",
    "    output_path=output_path,\n",
    "    base_job_name=\"hpo-medical-seg\",\n",
    "    enable_sagemaker_metrics=True,  # Required for HPO\n",
    "    metric_definitions=[\n",
    "        {\"Name\": \"val_dice\", \"Regex\": \"Validation Dice: ([0-9\\\\.]+)\"},\n",
    "        {\"Name\": \"train_loss\", \"Regex\": \"Training loss: ([0-9\\\\.]+)\"}\n",
    "    ],\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "print(\"‚úì Base estimator configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Hyperparameter Search Space\n",
    "\n",
    "Specify which hyperparameters to tune and their ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    # Learning rate: log scale between 1e-5 and 1e-3\n",
    "    \"lr\": ContinuousParameter(1e-5, 1e-3, scaling_type=\"Logarithmic\"),\n",
    "    \n",
    "    # Batch size: discrete choices\n",
    "    \"batch_size\": CategoricalParameter([2, 4, 8]),\n",
    "}\n",
    "\n",
    "print(\"üìä Hyperparameter Search Space:\")\n",
    "print(f\"  Learning Rate: 1e-5 to 1e-3 (log scale)\")\n",
    "print(f\"  Batch Size: [2, 4, 8]\")\n",
    "print(f\"\\nTotal combinations: Continuous √ó 3 = Infinite\")\n",
    "print(f\"Bayesian optimization will sample efficiently\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Configure Tuning Job\n",
    "\n",
    "Set up the hyperparameter tuner with objective metric and strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(\n",
    "    estimator=estimator,\n",
    "    objective_metric_name=\"val_dice\",\n",
    "    objective_type=\"Maximize\",  # Higher Dice score is better\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    metric_definitions=estimator.metric_definitions,\n",
    "    max_jobs=20,  # Total training jobs to run\n",
    "    max_parallel_jobs=2,  # Run 2 jobs simultaneously\n",
    "    strategy=\"Bayesian\",  # Bayesian optimization\n",
    "    early_stopping_type=\"Auto\"  # Stop poor performers early\n",
    ")\n",
    "\n",
    "print(\"‚úì Hyperparameter Tuner configured\")\n",
    "print(f\"  Objective: Maximize val_dice\")\n",
    "print(f\"  Strategy: Bayesian Optimization\")\n",
    "print(f\"  Max Jobs: 20\")\n",
    "print(f\"  Parallel Jobs: 2\")\n",
    "print(f\"  Early Stopping: Enabled\")\n",
    "print(f\"\\nEstimated Duration: 90-120 minutes\")\n",
    "print(f\"Estimated Cost: ~$20 (20 jobs √ó $1/job)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Launch Tuning Job\n",
    "\n",
    "Start the hyperparameter optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_job_name = f\"medhpo-{sagemaker.utils.sagemaker_timestamp()}\"\n",
    "\n",
    "tuner.fit(\n",
    "    inputs={\"training\": data_path},\n",
    "    job_name=tuning_job_name,\n",
    "    wait=False  # Don't block, we'll monitor separately\n",
    ")\n",
    "\n",
    "print(f\"\\nüöÄ Tuning job launched: {tuning_job_name}\")\n",
    "print(f\"\\nMonitor progress:\")\n",
    "print(f\"  Console: https://console.aws.amazon.com/sagemaker/home?region={region}#/hyper-tuning-jobs/{tuning_job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Monitor Tuning Progress\n",
    "\n",
    "Check the status and intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "\n",
    "def get_tuning_status():\n",
    "    response = sm_client.describe_hyper_parameter_tuning_job(\n",
    "        HyperParameterTuningJobName=tuning_job_name\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# Monitor for 5 minutes\n",
    "for i in range(10):\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    status = get_tuning_status()\n",
    "    \n",
    "    print(f\"‚è±Ô∏è  Tuning Job Status: {status['HyperParameterTuningJobStatus']}\")\n",
    "    print(f\"\\nüìä Job Counts:\")\n",
    "    print(f\"  Completed: {status['TrainingJobStatusCounters']['Completed']}\")\n",
    "    print(f\"  In Progress: {status['TrainingJobStatusCounters']['InProgress']}\")\n",
    "    print(f\"  Stopped: {status['TrainingJobStatusCounters']['Stopped']}\")\n",
    "    print(f\"  Failed: {status['TrainingJobStatusCounters'].get('NonRetryableError', 0)}\")\n",
    "    \n",
    "    if 'BestTrainingJob' in status:\n",
    "        best = status['BestTrainingJob']\n",
    "        print(f\"\\nüèÜ Current Best:\")\n",
    "        print(f\"  Job: {best['TrainingJobName']}\")\n",
    "        print(f\"  Dice Score: {best['FinalHyperParameterTuningJobObjectiveMetric']['Value']:.4f}\")\n",
    "    \n",
    "    if status['HyperParameterTuningJobStatus'] in ['Completed', 'Failed', 'Stopped']:\n",
    "        break\n",
    "    \n",
    "    time.sleep(30)\n",
    "\n",
    "print(\"\\n‚úì Monitoring complete. Run next cell to wait for completion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Wait for Completion (Optional)\n",
    "\n",
    "Block until all jobs finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will block until tuning completes\n",
    "tuner.wait()\n",
    "\n",
    "print(\"‚úì Hyperparameter tuning completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Analyze Results\n",
    "\n",
    "Retrieve and analyze all training jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get tuning job analytics\n",
    "tuning_analytics = sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name)\n",
    "df = tuning_analytics.dataframe()\n",
    "\n",
    "# Sort by objective metric\n",
    "df = df.sort_values('FinalObjectiveValue', ascending=False)\n",
    "\n",
    "print(\"üìä Top 10 Training Jobs:\")\n",
    "print(df[['TrainingJobName', 'FinalObjectiveValue', 'lr', 'batch_size', 'TrainingJobStatus']].head(10).to_string(index=False))\n",
    "\n",
    "# Best hyperparameters\n",
    "best_job = df.iloc[0]\n",
    "print(f\"\\nüèÜ Best Hyperparameters:\")\n",
    "print(f\"  Dice Score: {best_job['FinalObjectiveValue']:.4f}\")\n",
    "print(f\"  Learning Rate: {best_job['lr']:.6f}\")\n",
    "print(f\"  Batch Size: {int(best_job['batch_size'])}\")\n",
    "print(f\"  Job Name: {best_job['TrainingJobName']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Learning Rate vs Dice Score\n",
    "completed = df[df['TrainingJobStatus'] == 'Completed']\n",
    "axes[0].scatter(completed['lr'], completed['FinalObjectiveValue'], alpha=0.6, s=100)\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_xlabel('Learning Rate', fontsize=12)\n",
    "axes[0].set_ylabel('Dice Score', fontsize=12)\n",
    "axes[0].set_title('Learning Rate vs Performance', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Batch Size vs Dice Score\n",
    "batch_sizes = completed['batch_size'].unique()\n",
    "batch_means = [completed[completed['batch_size'] == bs]['FinalObjectiveValue'].mean() for bs in batch_sizes]\n",
    "axes[1].bar(batch_sizes, batch_means, alpha=0.7, color='steelblue')\n",
    "axes[1].set_xlabel('Batch Size', fontsize=12)\n",
    "axes[1].set_ylabel('Average Dice Score', fontsize=12)\n",
    "axes[1].set_title('Batch Size vs Performance', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('hpo_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualization saved: hpo_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Convergence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot best score over time\n",
    "df_sorted = df.sort_values('TrainingStartTime')\n",
    "df_sorted['BestSoFar'] = df_sorted['FinalObjectiveValue'].cummax()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(df_sorted)), df_sorted['BestSoFar'], marker='o', linewidth=2, markersize=6)\n",
    "plt.xlabel('Training Job Number', fontsize=12)\n",
    "plt.ylabel('Best Dice Score', fontsize=12)\n",
    "plt.title('Hyperparameter Optimization Convergence', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('hpo_convergence.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Calculate improvement\n",
    "initial_best = df_sorted['BestSoFar'].iloc[0]\n",
    "final_best = df_sorted['BestSoFar'].iloc[-1]\n",
    "improvement = ((final_best - initial_best) / initial_best) * 100\n",
    "\n",
    "print(f\"\\nüìà Optimization Progress:\")\n",
    "print(f\"  Initial Best: {initial_best:.4f}\")\n",
    "print(f\"  Final Best: {final_best:.4f}\")\n",
    "print(f\"  Improvement: {improvement:.2f}%\")\n",
    "print(f\"\\n‚úì Convergence plot saved: hpo_convergence.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Download Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best training job details\n",
    "best_training_job = sm_client.describe_training_job(\n",
    "    TrainingJobName=best_job['TrainingJobName']\n",
    ")\n",
    "\n",
    "model_artifacts = best_training_job['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "print(f\"Best Model Artifacts: {model_artifacts}\")\n",
    "\n",
    "# Download\n",
    "!aws s3 cp {model_artifacts} ./best_hpo_model.tar.gz\n",
    "!tar -xzf best_hpo_model.tar.gz\n",
    "\n",
    "print(\"\\n‚úì Best model downloaded\")\n",
    "!ls -lh *.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total cost\n",
    "total_time = df['TrainingElapsedTimeSeconds'].sum() / 3600  # hours\n",
    "cost_per_hour = 1.41  # ml.g5.xlarge\n",
    "total_cost = total_time * cost_per_hour\n",
    "\n",
    "completed_jobs = len(df[df['TrainingJobStatus'] == 'Completed'])\n",
    "stopped_jobs = len(df[df['TrainingJobStatus'] == 'Stopped'])\n",
    "\n",
    "print(\"üí∞ Cost Analysis:\")\n",
    "print(f\"  Total Training Time: {total_time:.2f} hours\")\n",
    "print(f\"  Instance Cost: ${cost_per_hour}/hour\")\n",
    "print(f\"  Total Cost: ${total_cost:.2f}\")\n",
    "print(f\"\\nüìä Job Statistics:\")\n",
    "print(f\"  Completed: {completed_jobs}\")\n",
    "print(f\"  Early Stopped: {stopped_jobs}\")\n",
    "print(f\"  Savings from Early Stopping: ~${stopped_jobs * 0.5:.2f}\")\n",
    "print(f\"\\nüí° Cost per 1% improvement: ${total_cost / improvement:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Bayesian Optimization\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Initial Exploration** (Jobs 1-5)\n",
    "   - Random sampling across search space\n",
    "   - Build initial surrogate model\n",
    "\n",
    "2. **Exploitation** (Jobs 6-15)\n",
    "   - Sample near best-performing regions\n",
    "   - Refine hyperparameter estimates\n",
    "\n",
    "3. **Final Refinement** (Jobs 16-20)\n",
    "   - Fine-tune around optimal values\n",
    "   - Confirm best configuration\n",
    "\n",
    "### Acquisition Functions\n",
    "\n",
    "SageMaker uses **Expected Improvement (EI)**:\n",
    "```\n",
    "EI(x) = E[max(f(x) - f(x_best), 0)]\n",
    "```\n",
    "\n",
    "Balances:\n",
    "- **Exploitation**: Sample where we expect high performance\n",
    "- **Exploration**: Sample where we're uncertain\n",
    "\n",
    "### Early Stopping\n",
    "\n",
    "Automatically stops jobs that are unlikely to beat current best:\n",
    "- Saves ~30-40% of training time\n",
    "- Based on learning curves\n",
    "- No manual intervention needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Tuning Strategies\n",
    "\n",
    "### 1. Multi-Objective Optimization\n",
    "```python\n",
    "# Optimize for both accuracy and speed\n",
    "objective_metric_name=\"val_dice\"\n",
    "# Add secondary metric in analysis\n",
    "```\n",
    "\n",
    "### 2. Warm Start Tuning\n",
    "```python\n",
    "# Continue from previous tuning job\n",
    "tuner = HyperparameterTuner(\n",
    "    ...,\n",
    "    warm_start_config=WarmStartConfig(\n",
    "        warm_start_type=WarmStartTypes.IDENTICAL_DATA_AND_ALGORITHM,\n",
    "        parents=[previous_tuning_job_name]\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "### 3. Transfer Learning\n",
    "```python\n",
    "# Use results from similar task\n",
    "warm_start_type=WarmStartTypes.TRANSFER_LEARNING\n",
    "```\n",
    "\n",
    "### 4. Expanded Search Space\n",
    "```python\n",
    "hyperparameter_ranges = {\n",
    "    \"lr\": ContinuousParameter(1e-5, 1e-3),\n",
    "    \"batch_size\": CategoricalParameter([2, 4, 8]),\n",
    "    \"model_name\": CategoricalParameter([\"SegResNet\", \"UNet\"]),\n",
    "    \"weight_decay\": ContinuousParameter(1e-6, 1e-3),\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "‚úì **Bayesian Optimization Benefits:**\n",
    "- 3-5x more efficient than random search\n",
    "- Learns from previous results\n",
    "- Automatic early stopping saves 30-40% cost\n",
    "- Typically finds near-optimal in 20-30 jobs\n",
    "\n",
    "‚úì **Best Practices:**\n",
    "- Start with 2-3 hyperparameters\n",
    "- Use log scale for learning rates\n",
    "- Run 20-30 jobs minimum\n",
    "- Enable early stopping\n",
    "- Monitor convergence plots\n",
    "\n",
    "‚úì **When to Use HPO:**\n",
    "- New dataset or domain\n",
    "- Production model optimization\n",
    "- When 1-2% improvement matters\n",
    "- Budget allows multiple training runs\n",
    "\n",
    "‚úì **When NOT to Use HPO:**\n",
    "- Prototyping phase\n",
    "- Well-known hyperparameters\n",
    "- Limited budget\n",
    "- Quick experiments\n",
    "\n",
    "## Typical Results\n",
    "\n",
    "| Metric | Manual Tuning | HPO (20 jobs) |\n",
    "|--------|---------------|---------------|\n",
    "| Best Dice | 0.82 | 0.85 |\n",
    "| Time Spent | 2-3 days | 2 hours |\n",
    "| Cost | $10-15 | $20 |\n",
    "| Reproducibility | Low | High |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Deploy best model to SageMaker Endpoint\n",
    "- Run warm-start tuning with expanded search space\n",
    "- Try multi-objective optimization\n",
    "- Automate HPO in ML pipeline\n",
    "- Document optimal hyperparameters for team\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [SageMaker HPO Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html)\n",
    "- [Bayesian Optimization Paper](https://arxiv.org/abs/1807.02811)\n",
    "- [HPO Best Practices](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-considerations.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seg-jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
