{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Tutorial: Medical Image Splitting\n",
    "\n",
    "Split medical imaging datasets into train/test/validation sets using SageMaker Processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Dockerfile\n",
    "Here is the Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM ubuntu:latest\n",
    "\n",
    "RUN apt update\n",
    "RUN apt install python3 -y\n",
    "\n",
    "ENTRYPOINT [\"python3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and push the Docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build_and_push.sh\n",
    "#!/bin/bash\n",
    "AWS_ACCOUNT_ID=$1\n",
    "AWS_REGION=$2\n",
    "AWS_ECR_REPO=sm-preprocessing\n",
    "\n",
    "IMAGE=$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$AWS_ECR_REPO:latest\n",
    "\n",
    "docker build -t $IMAGE .\n",
    "# docker run -it -p 8080:8080 $IMAGE \n",
    "aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com\n",
    "# Push the image to ECR\n",
    "\n",
    "aws ecr create-repository --repository-name $AWS_ECR_REPO\n",
    "\n",
    "docker push $IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command\n",
    "\n",
    "`bash build_and_push.sh <Your_account_id> <region>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash build_and_push.sh xxxxxxxxx us-east-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "Import SageMaker libraries and configure session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, ScriptProcessor\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "\n",
    "# Use default session (us-east-1)\n",
    "sagemaker_session = sagemaker.Session()\n",
    "execution_role = get_execution_role()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "print(f\"Role: {execution_role}\")\n",
    "print(f\"Region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure Data Paths\n",
    "\n",
    "Set S3 paths for input and output data.\n",
    "\n",
    "### Dataset Structure\n",
    "```\n",
    "data\n",
    "├── class_1\n",
    "│   ├── image_1.png\n",
    "│   ├── image_2.png\n",
    "│   └── ...\n",
    "├── class_2\n",
    "│   ├── image_1.png\n",
    "│   ├── image_2.png\n",
    "│   └── ...\n",
    "└── ...\n",
    "```\n",
    "Each class folder contains images belonging to that class.\n",
    "\n",
    "### Processed dataset\n",
    "```\n",
    "data\n",
    "├── train\n",
    "│   ├── class_1\n",
    "│   │   ├── image_1.png\n",
    "│   │   ├── image_2.png\n",
    "│   │   └── ...\n",
    "│   ├── class_2\n",
    "│   │   ├── image_1.png\n",
    "│   │   ├── image_2.png\n",
    "│   │   └── ...\n",
    "│   └── ...\n",
    "├── validation\n",
    "│   ├── class_1\n",
    "│   │   ├── image_1.png\n",
    "│   │   ├── image_2.png\n",
    "│   │   └── ...\n",
    "│   ├── class_2\n",
    "│   │   ├── image_1.png\n",
    "│   │   ├── image_2.png \n",
    "│   │   └── ...\n",
    "│   └── ...\n",
    "└── test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 paths to be changed as per your setup\n",
    "input_bucket_name = \"Enter-Your-Bucket-Name-Here\"\n",
    "dataset_name = 'enter-dataset-name-here'\n",
    "output_dataset_name = 'processed-data'\n",
    "input_s3_uri = f's3://{input_bucket_name}/{dataset_name}'\n",
    "output_s3_uri = f's3://{input_bucket_name}/{output_dataset_name}'\n",
    "\n",
    "print(f\"Input: {input_s3_uri}\")\n",
    "print(f\"Output: {output_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Preprocessing Script\n",
    "\n",
    "Write script to split data by class into train/test/val sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import shutil\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logging.getLogger().info(\"Starting preprocessing script\")\n",
    "\"\"\"\n",
    "The dataset is distributed in the following way. The source directory contains all the images in individual class folders. \n",
    "We need to split the data into train, test and validation sets. In each of the train, test and validation sets, we need to have the same distribution of classes.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    data_src = '/opt/ml/processing/input'\n",
    "    data_dest = '/opt/ml/processing/output'\n",
    "    \n",
    "    list_classes = os.listdir(data_src)\n",
    "    list_classes.sort()\n",
    "    logging.info(f\"Classes found: {list_classes}\")\n",
    "    for class_name in list_classes:\n",
    "        os.makedirs(os.path.join(data_dest, 'train', class_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(data_dest, 'test', class_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(data_dest, 'val', class_name), exist_ok=True)\n",
    "        \n",
    "    # Split the data into train, test and validation sets\n",
    "    for class_name in list_classes:\n",
    "        list_images = os.listdir(os.path.join(data_src, class_name))\n",
    "        list_images.sort()\n",
    "        train_images = list_images[:int(len(list_images)*0.7)]\n",
    "        test_images = list_images[int(len(list_images)*0.7):int(len(list_images)*0.9)]\n",
    "        val_images = list_images[int(len(list_images)*0.9):]\n",
    "        for image in train_images:\n",
    "            # copy the image to the train folder\n",
    "            shutil.copy(os.path.join(data_src, class_name, image), os.path.join(data_dest, 'train', class_name, image))\n",
    "        for image in test_images:\n",
    "            shutil.copy(os.path.join(data_src, class_name, image), os.path.join(data_dest, 'test', class_name, image))\n",
    "        for image in val_images:\n",
    "            shutil.copy(os.path.join(data_src, class_name, image), os.path.join(data_dest, 'val', class_name, image))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Script Processor\n",
    "\n",
    "Configure SageMaker processor with container image and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SageMaker Python container\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, ScriptProcessor\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "\n",
    "region='us-east-1'\n",
    "image_uri = f'683313688378.dkr.ecr.{region}.amazonaws.com/sagemaker-scikit-learn:1.2-1-cpu-py3'\n",
    "\n",
    "print(image_uri)\n",
    "script_processor = ScriptProcessor(\n",
    "    command=['python3'],\n",
    "    image_uri=image_uri,\n",
    "    role=execution_role,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    instance_count=1,\n",
    ")\n",
    "\n",
    "print(f\"Using image: {image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Processing Job\n",
    "\n",
    "Execute data splitting with custom split ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Generate unique job name\n",
    "job_name = f'data-splitting-{int(time.time())}'\n",
    "\n",
    "script_processor.run(\n",
    "    code='preprocessing.py',\n",
    "    job_name=job_name,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=input_s3_uri, \n",
    "            destination='/opt/ml/processing/input'\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            source='/opt/ml/processing/output/train', \n",
    "            destination=output_s3_uri + '/train'\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            source='/opt/ml/processing/output/test', \n",
    "            destination=output_s3_uri + '/test'\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            source='/opt/ml/processing/output/val', \n",
    "            destination=output_s3_uri + '/val'\n",
    "        )\n",
    "    ],\n",
    "    arguments=['--train-split', '0.7', '--test-split', '0.15', '--val-split', '0.15']\n",
    ")\n",
    "\n",
    "print(f\"Processing job {job_name} completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Verify Output\n",
    "\n",
    "Check processed data in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# List output folders\n",
    "for split in ['train', 'test', 'val']:\n",
    "    response = s3.list_objects_v2(\n",
    "        Bucket=input_bucket_name,\n",
    "        Prefix=f'{output_dataset_name}/{split}/',\n",
    "        MaxKeys=10\n",
    "    )\n",
    "    count = response.get('KeyCount', 0)\n",
    "    print(f\"{split}: {count} objects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Data is now split and ready for training. Use the processed data paths in the EC2 training notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop_singleGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
